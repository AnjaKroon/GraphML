{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from gtda.graphs import KNeighborsGraph\n",
    "import itertools\n",
    "from utils import GraphRNN_dataset, GraphRNN_DataSampler\n",
    "import random\n",
    "\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessor import Preprocessor\n",
    "from preprocessor import draw_network\n",
    "from preprocessor import get_adj_from_plot\n",
    "\n",
    "from preprocessor_final_data import Preprocessor\n",
    "from preprocessor_final_data import draw_network\n",
    "from preprocessor_final_data import get_adj_from_plot\n",
    "\n",
    "import preprocessor_final_data \n",
    "import os\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from Model3 import GraphConvolutionalNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gathering data...\")\n",
    "flow_dataset = \"../data/daily_county2county_2019_01_01.csv\"\n",
    "epi_dataset = \"../data_epi/epidemiology.csv\"\n",
    "epi_dates = [\"2020-06-09\", \"2020-06-10\", \"2020-06-11\", \"2020-06-12\",\n",
    "             \"2020-06-13\", \"2020-06-14\", \"2020-06-15\", \"2020-06-16\",\n",
    "             \"2020-06-17\", \"2020-06-18\", \"2020-06-19\", \"2020-06-20\",\n",
    "             \"2020-06-21\", \"2020-06-22\", \"2020-06-23\", \"2020-06-24\",\n",
    "                \"2020-06-25\", \"2020-06-26\", \"2020-06-27\", \"2020-06-28\",\n",
    "                \"2020-06-29\", \"2020-06-30\", \"2020-07-01\", \"2020-07-02\",\n",
    "                \"2020-07-03\", \"2020-07-04\", \"2020-07-05\", \"2020-07-06\",\n",
    "                \"2020-07-07\", \"2020-07-08\", \"2020-07-09\", \"2020-07-10\",\n",
    "                \"2020-07-11\", \"2020-07-12\", \"2020-07-13\", \"2020-07-14\",\n",
    "                \"2020-07-15\", \"2020-07-16\", \"2020-07-17\", \"2020-07-18\",\n",
    "                \"2020-07-19\", \"2020-07-20\", \"2020-07-21\", \"2020-07-22\",\n",
    "                \"2020-07-23\", \"2020-07-24\", \"2020-07-25\", \"2020-07-26\",\n",
    "                \"2020-07-27\", \"2020-07-28\", \"2020-07-29\", \"2020-07-30\"\n",
    "             ]\n",
    "\n",
    "epi_dates_medium = [\n",
    "    \"2020-06-09\", \"2020-06-10\", \"2020-06-11\", \"2020-06-12\",\n",
    "    \"2020-06-13\", \"2020-06-14\", \"2020-06-15\", \"2020-06-16\",\n",
    "    \"2020-06-17\", \"2020-06-18\", \"2020-06-19\", \"2020-06-20\",\n",
    "    \"2020-06-21\", \"2020-06-22\", \"2020-06-23\", \"2020-06-24\",\n",
    "    \"2020-06-25\", \"2020-06-26\", \"2020-06-27\", \"2020-06-28\",\n",
    "    \"2020-06-29\", \"2020-06-30\", \"2020-07-01\", \"2020-07-02\",\n",
    "    \"2020-07-03\", \"2020-07-04\", \"2020-07-05\", \"2020-07-06\",\n",
    "    \"2020-07-07\", \"2020-07-08\", \"2020-07-09\", \"2020-07-10\",\n",
    "    \"2020-07-11\", \"2020-07-12\", \"2020-07-13\", \"2020-07-14\",\n",
    "    \"2020-07-15\", \"2020-07-16\", \"2020-07-17\", \"2020-07-18\",\n",
    "    \"2020-07-19\", \"2020-07-20\", \"2020-07-21\", \"2020-07-22\",\n",
    "    \"2020-07-23\", \"2020-07-24\", \"2020-07-25\", \"2020-07-26\",\n",
    "    \"2020-07-27\", \"2020-07-28\", \"2020-07-29\", \"2020-07-30\",\n",
    "    \"2020-07-31\", \"2020-08-01\", \"2020-08-02\", \"2020-08-03\",\n",
    "    \"2020-08-04\", \"2020-08-05\", \"2020-08-06\", \"2020-08-07\",\n",
    "    \"2020-08-08\", \"2020-08-09\", \"2020-08-10\", \"2020-08-11\",\n",
    "    \"2020-08-12\", \"2020-08-13\", \"2020-08-14\", \"2020-08-15\",\n",
    "    \"2020-08-16\", \"2020-08-17\", \"2020-08-18\", \"2020-08-19\",\n",
    "    \"2020-08-20\", \"2020-08-21\", \"2020-08-22\", \"2020-08-23\",\n",
    "    \"2020-08-24\", \"2020-08-25\", \"2020-08-26\", \"2020-08-27\",\n",
    "    \"2020-08-28\", \"2020-08-29\", \"2020-08-30\", \"2020-08-31\",\n",
    "    \"2020-09-01\", \"2020-09-02\", \"2020-09-03\", \"2020-09-04\",\n",
    "    \"2020-09-05\", \"2020-09-06\", \"2020-09-07\", \"2020-09-08\",\n",
    "    \"2020-09-09\", \"2020-09-10\", \"2020-09-11\", \"2020-09-12\",\n",
    "    \"2020-09-13\", \"2020-09-14\", \"2020-09-15\", \"2020-09-16\",\n",
    "    \"2020-09-17\", \"2020-09-18\", \"2020-09-19\", \"2020-09-20\",\n",
    "    \"2020-09-21\", \"2020-09-22\", \"2020-09-23\", \"2020-09-24\",\n",
    "    \"2020-09-25\", \"2020-09-26\", \"2020-09-27\", \"2020-09-28\",\n",
    "    \"2020-09-29\", \"2020-09-30\"\n",
    "]\n",
    "\n",
    "epi_dates_large = [\n",
    "    \"2020-06-01\", \"2020-06-02\", \"2020-06-03\", \"2020-06-04\", \n",
    "    \"2020-06-05\", \"2020-06-06\", \"2020-06-07\", \"2020-06-08\",\n",
    "    \"2020-06-09\", \"2020-06-10\", \"2020-06-11\", \"2020-06-12\",\n",
    "    \"2020-06-13\", \"2020-06-14\", \"2020-06-15\", \"2020-06-16\",\n",
    "    \"2020-06-17\", \"2020-06-18\", \"2020-06-19\", \"2020-06-20\",\n",
    "    \"2020-06-21\", \"2020-06-22\", \"2020-06-23\", \"2020-06-24\",\n",
    "    \"2020-06-25\", \"2020-06-26\", \"2020-06-27\", \"2020-06-28\",\n",
    "    \"2020-06-29\", \"2020-06-30\", \"2020-07-01\", \"2020-07-02\",\n",
    "    \"2020-07-03\", \"2020-07-04\", \"2020-07-05\", \"2020-07-06\",\n",
    "    \"2020-07-07\", \"2020-07-08\", \"2020-07-09\", \"2020-07-10\",\n",
    "    \"2020-07-11\", \"2020-07-12\", \"2020-07-13\", \"2020-07-14\",\n",
    "    \"2020-07-15\", \"2020-07-16\", \"2020-07-17\", \"2020-07-18\",\n",
    "    \"2020-07-19\", \"2020-07-20\", \"2020-07-21\", \"2020-07-22\",\n",
    "    \"2020-07-23\", \"2020-07-24\", \"2020-07-25\", \"2020-07-26\",\n",
    "    \"2020-07-27\", \"2020-07-28\", \"2020-07-29\", \"2020-07-30\",\n",
    "    \"2020-07-31\", \"2020-08-01\", \"2020-08-02\", \"2020-08-03\",\n",
    "    \"2020-08-04\", \"2020-08-05\", \"2020-08-06\", \"2020-08-07\",\n",
    "    \"2020-08-08\", \"2020-08-09\", \"2020-08-10\", \"2020-08-11\",\n",
    "    \"2020-08-12\", \"2020-08-13\", \"2020-08-14\", \"2020-08-15\",\n",
    "    \"2020-08-16\", \"2020-08-17\", \"2020-08-18\", \"2020-08-19\",\n",
    "    \"2020-08-20\", \"2020-08-21\", \"2020-08-22\", \"2020-08-23\",\n",
    "    \"2020-08-24\", \"2020-08-25\", \"2020-08-26\", \"2020-08-27\",\n",
    "    \"2020-08-28\", \"2020-08-29\", \"2020-08-30\", \"2020-08-31\",\n",
    "    \"2020-09-01\", \"2020-09-02\", \"2020-09-03\", \"2020-09-04\",\n",
    "    \"2020-09-05\", \"2020-09-06\", \"2020-09-07\", \"2020-09-08\",\n",
    "    \"2020-09-09\", \"2020-09-10\", \"2020-09-11\", \"2020-09-12\",\n",
    "    \"2020-09-13\", \"2020-09-14\", \"2020-09-15\", \"2020-09-16\",\n",
    "    \"2020-09-17\", \"2020-09-18\", \"2020-09-19\", \"2020-09-20\",\n",
    "    \"2020-09-21\", \"2020-09-22\", \"2020-09-23\", \"2020-09-24\",\n",
    "    \"2020-09-25\", \"2020-09-26\", \"2020-09-27\", \"2020-09-28\",\n",
    "    \"2020-09-29\", \"2020-09-30\", \"2020-10-01\", \"2020-10-02\",\n",
    "    \"2020-10-03\", \"2020-10-04\", \"2020-10-05\", \"2020-10-06\",\n",
    "    \"2020-10-07\", \"2020-10-08\", \"2020-10-09\", \"2020-10-10\",\n",
    "    \"2020-10-11\", \"2020-10-12\", \"2020-10-13\", \"2020-10-14\",\n",
    "    \"2020-10-15\", \"2020-10-16\", \"2020-10-17\", \"2020-10-18\",\n",
    "    \"2020-10-19\", \"2020-10-20\", \"2020-10-21\", \"2020-10-22\",\n",
    "    \"2020-10-23\", \"2020-10-24\", \"2020-10-25\", \"2020-10-26\",\n",
    "    \"2020-10-27\", \"2020-10-28\", \"2020-10-29\", \"2020-10-30\",\n",
    "    \"2020-10-31\", \"2020-11-01\", \"2020-11-02\", \"2020-11-03\",\n",
    "    \"2020-11-04\", \"2020-11-05\", \"2020-11-06\", \"2020-11-07\",\n",
    "    \"2020-11-08\", \"2020-11-09\", \"2020-11-10\", \"2020-11-11\",\n",
    "    \"2020-11-12\", \"2020-11-13\", \"2020-11-14\", \"2020-11-15\",\n",
    "    \"2020-11-16\", \"2020-11-17\", \"2020-11-18\", \"2020-11-19\",\n",
    "    \"2020-11-20\", \"2020-11-21\", \"2020-11-22\", \"2020-11-23\",\n",
    "    \"2020-11-24\", \"2020-11-25\", \"2020-11-26\", \"2020-11-27\",\n",
    "    \"2020-11-28\", \"2020-11-29\", \"2020-11-30\", \"2020-12-01\",\n",
    "    \"2020-12-02\", \"2020-12-03\", \"2020-12-04\", \"2020-12-05\",\n",
    "    \"2020-12-06\", \"2020-12-07\", \"2020-12-08\", \"2020-12-09\",\n",
    "    \"2020-12-10\", \"2020-12-11\", \"2020-12-12\", \"2020-12-13\",\n",
    "    \"2020-12-14\", \"2020-12-15\", \"2020-12-16\", \"2020-12-17\",\n",
    "    \"2020-12-18\", \"2020-12-19\", \"2020-12-20\", \"2020-12-21\",\n",
    "    \"2020-12-22\", \"2020-12-23\", \"2020-12-24\", \"2020-12-25\",\n",
    "    \"2020-12-26\", \"2020-12-27\", \"2020-12-28\", \"2020-12-29\",\n",
    "    \"2020-12-30\", \"2020-12-31\"\n",
    "]\n",
    "\n",
    "epi_dates_smaller = [\"2020-06-09\", \"2020-06-10\", \"2020-06-11\", \"2020-06-12\",\n",
    "             \"2020-06-13\", \"2020-06-14\", \"2020-06-15\", \"2020-06-16\",\n",
    "             \"2020-06-17\", \"2020-06-18\", \"2020-06-19\", \"2020-06-20\",\n",
    "             ]\n",
    "\n",
    "epi_dates_one_sample = [\"2020-06-09\", \"2020-06-10\", \"2020-06-11\", \"2020-06-12\",\n",
    "             \"2020-06-13\", \"2020-06-14\", \"2020-06-15\"] # for testing\n",
    "epi_dates_pred_one_sample = [\"2020-06-16\"] # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS WITH THE NEW PREPROCESSOR FILE (PREPROCESSOR_FINAL_DATA.PY)\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Preprocess the data\n",
    "input_hor = 7   # test now with smaller input horizon\n",
    "pred_hor = 1    # test now with smaller prediction horizon\n",
    "train_perc, test_perc = 0.8, 0.2\n",
    "\n",
    "# Preprocess the data\n",
    "print(os.getcwd())\n",
    "locations_data = \"../final_data/locations_data_unique.npy\"\n",
    "epi_dataset = \"../final_data/X_normalized.npy\"\n",
    "preproc = preprocessor_final_data.Preprocessor(flow_dataset, epi_dataset, locations_data, plottable=True)\n",
    "\n",
    "graph_kronecker_whole_df = preproc.combined_manual_kronecker() # makes the pandas df from the kronecker data\n",
    "\n",
    "# check for duplicate values in graph_kronecker_whole_df\n",
    "print(\"Number of duplicate values in graph_kronecker_whole_df: \", graph_kronecker_whole_df.duplicated().sum())\n",
    "\n",
    "# print first dataframe in graph_kronecker_whole_df, check if the data is getting pulled correctly\n",
    "print(\"Check: First dataframe in graph_kronecker_whole_df: \", graph_kronecker_whole_df.iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEWER VERSION\n",
    "\n",
    "print(\"Getting the training graph signal...\")\n",
    "# do it first only with cumulative_confirmed\n",
    "tr_epi = preproc.set_timestep_offset_epi_dataset(from_timestep=0).get_epi_dataset() # will get the epi info for the entire dataset, the entire kronecker, then later we index per training example and pred\n",
    "print(len(tr_epi)) # working, should be 30\n",
    "print(tr_epi[0]) # working, should be the geoids of the first day\n",
    "\n",
    "train_graph_sig = {}\n",
    "for day in tr_epi:\n",
    "    entry_count = 0\n",
    "    for geoid in day.geoid_o:\n",
    "        if geoid not in train_graph_sig: # which should be for every day\n",
    "            train_graph_sig[geoid] = {\n",
    "                'weather1': [],\n",
    "                'weather2': [],\n",
    "                'cumulative_confirmed': [],\n",
    "                'cumulative_deceased': [],\n",
    "                'new_deceased': [],\n",
    "                'cumulative_persons_fully_vaccinated': [],\n",
    "                'new_persons_fully_vaccinated': []\n",
    "            }\n",
    "\n",
    "        train_graph_sig[geoid]['weather1'].append(day.weather1[entry_count])\n",
    "        train_graph_sig[geoid]['weather2'].append(day.weather2[entry_count])\n",
    "        train_graph_sig[geoid]['cumulative_confirmed'].append(day.cumulative_confirmed[entry_count])\n",
    "        train_graph_sig[geoid]['cumulative_deceased'].append(day.cumulative_deceased[entry_count])\n",
    "        train_graph_sig[geoid]['new_deceased'].append(day.new_deceased[entry_count])\n",
    "        train_graph_sig[geoid]['cumulative_persons_fully_vaccinated'].append(day.cumulative_persons_fully_vaccinated[entry_count])\n",
    "        train_graph_sig[geoid]['new_persons_fully_vaccinated'].append(day.new_persons_fully_vaccinated[entry_count])\n",
    "        entry_count += 1\n",
    "print(\"Check: Should be number of nodes per day * number of days\", len(train_graph_sig))  # working\n",
    "\n",
    "# sort train_graph_sig by geoid_o\n",
    "train_graph_sig = dict(sorted(train_graph_sig.items(), key=lambda item: item[0]))\n",
    "\n",
    "# print the first element of train_graph_sig, check if the data is getting pulled correctly\n",
    "print(\"Check: First element in train_graph_sig: \", {k: train_graph_sig[k] for k in list(train_graph_sig)[:1]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE \n",
    "\n",
    "# Combine all values for each feature across all geo_ids\n",
    "weather1_values = []\n",
    "weather2_values = []\n",
    "cumulative_confirmed_values = []\n",
    "cumulative_deceased_values = []\n",
    "new_deceased_values = []\n",
    "cumulative_persons_fully_vaccinated_values = []\n",
    "new_persons_fully_vaccinated_values = []\n",
    "\n",
    "for geoid in train_graph_sig:\n",
    "    weather1_values.extend(train_graph_sig[geoid]['weather1'])\n",
    "    weather2_values.extend(train_graph_sig[geoid]['weather2'])\n",
    "    cumulative_confirmed_values.extend(train_graph_sig[geoid]['cumulative_confirmed'])\n",
    "    cumulative_deceased_values.extend(train_graph_sig[geoid]['cumulative_deceased'])\n",
    "    new_deceased_values.extend(train_graph_sig[geoid]['new_deceased'])\n",
    "    cumulative_persons_fully_vaccinated_values.extend(train_graph_sig[geoid]['cumulative_persons_fully_vaccinated'])\n",
    "    new_persons_fully_vaccinated_values.extend(train_graph_sig[geoid]['new_persons_fully_vaccinated'])\n",
    "\n",
    "# Calculate mean and std for each feature across all geo_ids\n",
    "mean_weather1 = np.mean(weather1_values)\n",
    "std_weather1 = np.std(weather1_values)\n",
    "\n",
    "mean_weather2 = np.mean(weather2_values)\n",
    "std_weather2 = np.std(weather2_values)\n",
    "\n",
    "mean_cumulative_confirmed = np.mean(cumulative_confirmed_values)\n",
    "std_cumulative_confirmed = np.std(cumulative_confirmed_values)\n",
    "\n",
    "mean_cumulative_deceased = np.mean(cumulative_deceased_values)\n",
    "std_cumulative_deceased = np.std(cumulative_deceased_values)\n",
    "\n",
    "mean_new_deceased = np.mean(new_deceased_values)\n",
    "std_new_deceased = np.std(new_deceased_values)\n",
    "\n",
    "mean_cumulative_persons_fully_vaccinated = np.mean(cumulative_persons_fully_vaccinated_values)\n",
    "std_cumulative_persons_fully_vaccinated = np.std(cumulative_persons_fully_vaccinated_values)\n",
    "\n",
    "mean_new_persons_fully_vaccinated = np.mean(new_persons_fully_vaccinated_values)\n",
    "std_new_persons_fully_vaccinated = np.std(new_persons_fully_vaccinated_values)\n",
    "\n",
    "# Normalize each feature list in train_graph_sig across all geo_ids\n",
    "for geoid in train_graph_sig:\n",
    "    train_graph_sig[geoid]['weather1'] = (train_graph_sig[geoid]['weather1'] - mean_weather1) / std_weather1 if std_weather1 != 0 else 0\n",
    "    train_graph_sig[geoid]['weather2'] = (train_graph_sig[geoid]['weather2'] - mean_weather2) / std_weather2 if std_weather2 != 0 else 0\n",
    "    train_graph_sig[geoid]['cumulative_confirmed'] = (train_graph_sig[geoid]['cumulative_confirmed'] - mean_cumulative_confirmed) / std_cumulative_confirmed if std_cumulative_confirmed != 0 else 0\n",
    "    train_graph_sig[geoid]['cumulative_deceased'] = (train_graph_sig[geoid]['cumulative_deceased'] - mean_cumulative_deceased) / std_cumulative_deceased if std_cumulative_deceased != 0 else 0\n",
    "    train_graph_sig[geoid]['new_deceased'] = (train_graph_sig[geoid]['new_deceased'] - mean_new_deceased) / std_new_deceased if std_new_deceased != 0 else 0\n",
    "    train_graph_sig[geoid]['cumulative_persons_fully_vaccinated'] = (train_graph_sig[geoid]['cumulative_persons_fully_vaccinated'] - mean_cumulative_persons_fully_vaccinated) / std_cumulative_persons_fully_vaccinated if std_cumulative_persons_fully_vaccinated != 0 else 0\n",
    "    train_graph_sig[geoid]['new_persons_fully_vaccinated'] = (train_graph_sig[geoid]['new_persons_fully_vaccinated'] - mean_new_persons_fully_vaccinated) / std_new_persons_fully_vaccinated if std_new_persons_fully_vaccinated != 0 else 0\n",
    "\n",
    "# print the first element of train_graph_sig, check if the data is getting pulled correctly\n",
    "print(\"Check: First element in train_graph_sig: \", {k: train_graph_sig[k] for k in list(train_graph_sig)[:1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_examples = []\n",
    "\n",
    "adj_kronecker_whole = get_adj_from_plot(graph_kronecker_whole_df)\n",
    "list_of_geoids = preproc.flow.iloc[:]['geoid_o'].unique()\n",
    "num_nodes_per_day = len(list_of_geoids)\n",
    "\n",
    "for example_num in range(0, len(tr_epi) - (input_hor)): # this is how many times you can \"shift and have valid data to pull from\"\n",
    "    # now draw out the adjacency matrix per example\n",
    "    width_of_adj_per_example = num_nodes_per_day*input_hor\n",
    "    shift = num_nodes_per_day # number of nodes per day\n",
    "\n",
    "    # this should still work in the new approach\n",
    "    adj_per_example = adj_kronecker_whole[ (example_num*shift) : (example_num*shift) + width_of_adj_per_example,\n",
    "                                           (example_num*shift) : (example_num*shift) + width_of_adj_per_example]\n",
    "\n",
    "    # now drawing out the train_graph_signal per example\n",
    "\n",
    "    # get the graph signal corresponding to the example nodes\n",
    "    offset = (example_num * shift)\n",
    "    train_graph_sig_per_example = {k: train_graph_sig[k] for k in \\\n",
    "                                   list(train_graph_sig)[offset : offset + width_of_adj_per_example]}\n",
    "\n",
    "    # get the graph signal corresponding to the [input_hor : input_hor+pred_hor] set of nodes -- NEEDS CHECKING\n",
    "    \n",
    "    train_graph_sig_per_example_pred = {k: train_graph_sig[k] for k in \\\n",
    "                                        list(train_graph_sig)[offset + (num_nodes_per_day * input_hor) : offset + num_nodes_per_day*(input_hor + pred_hor)]}\n",
    "\n",
    "    \n",
    "    training_example = [adj_per_example, train_graph_sig_per_example, train_graph_sig_per_example_pred]\n",
    "    all_training_examples.append(training_example)\n",
    "\n",
    "# take the last 20% of the the training examples and use them as test examples\n",
    "all_test_examples = all_training_examples[int(len(all_training_examples)*train_perc):]\n",
    "# remove those examples from all_training_examples\n",
    "all_training_examples = all_training_examples[:int(len(all_training_examples)*train_perc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the training_data and testing_data in a pickle file\n",
    "import pickle\n",
    "with open(\"training_data.pkl\", \"wb\") as f:\n",
    "   pickle.dump(all_training_examples, f)\n",
    "\n",
    "with open(\"testing_data.pkl\", \"wb\") as f:\n",
    "   pickle.dump(all_test_examples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle files\n",
    "import pickle\n",
    "with open(\"training_data.pkl\", \"rb\") as f:\n",
    "   training_data = pickle.load(f)\n",
    "\n",
    "with open(\"testing_data.pkl\", \"rb\") as f:\n",
    "    testing_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH CONVOLUTIONAL NETWORK BASED ON KRONECKER GRAPH -- TOO MANY PARAMS NOW, TALK TO PEOPLE ABOUT THIS & HOW TO REDUCE\n",
    "\n",
    "# Define the model parameters\n",
    "\n",
    "num_features = len(list(train_graph_sig.values())[0])\n",
    "input_dim = num_features  # Number of features per node\n",
    "output_dim = num_features  # Number of output features per node\n",
    "hidden_dim_low = 70\n",
    "hidden_dim_high = 150\n",
    "num_nodes_kron = training_data[0][0].shape[0]\n",
    "print(\"Number of nodes in the kronecker graph: \", num_nodes_kron)\n",
    "num_nodes_pred = len(training_data[0][2])\n",
    "print(num_nodes_kron)\n",
    "print(num_nodes_pred)\n",
    "\n",
    "desired_num_params = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model3\n",
    "reload(Model3)\n",
    "from Model3 import GraphConvolutionalNetwork\n",
    "\n",
    "#  Instantiate the model\n",
    "input_horizon, prediction_horizon = 0, 0 # TODO amke use of these\n",
    "model = GraphConvolutionalNetwork(desired_num_params, input_horizon, prediction_horizon, num_features, input_dim, output_dim, num_nodes_kron, num_nodes_pred)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters:\", num_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "num_epochs = 100\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# shuffle the training data randomly\n",
    "print( training_data[0][0].shape)\n",
    "random.shuffle(training_data)\n",
    "random.shuffle(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING WITH NEW PREPROCESSOR\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for data in training_data:\n",
    "        output = model(data) # needs to have shape [num_nodes_pred, num_features]\n",
    "        # output is of shape [3070] turn into shape [3070, 1] \n",
    "        # output = output.view(-1, 1) # I dont think i need because now it is a multidimensional output\n",
    "        \n",
    "        loss = model.getLoss(output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.8f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for data in testing_data:\n",
    "        output = model(data) \n",
    "\n",
    "        loss = model.getLoss(output)\n",
    "        test_loss += loss.item()\n",
    "    test_loss /= len(testing_data)\n",
    "\n",
    "# Compute metrics\n",
    "print(f'Test Loss: {test_loss:.8f}')\n",
    "print(model.target_graph_signal_matrix[:, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output) # last test output\n",
    "#print(target_graph_signal) # last test example\n",
    "\n",
    "# plot the united states and have these values as the nodes\n",
    "\n",
    "# get a list of the key values in train_graph_sig\n",
    "geoid_list_tr = list(train_graph_sig.keys())\n",
    "geoid_list_tar = list(target_graph_signal.keys())\n",
    "\n",
    "# make a dictionary where key is geoid_list entry and value is out entry\n",
    "out_dict = {geoid_list_tr[i]: output[i, 2].item() for i in range(len(output))}\n",
    "\n",
    "target_dict = {i: target_graph_signal[i]['cumulative_confirmed'] for i in geoid_list_tar}\n",
    "\n",
    "# need to get the geographical information on where to put these nodes\n",
    "# get the geographical information from the preprocessor, 'geoid_o', 'lat_o', 'lng_o' for all the geoids in out_dict\n",
    "#print(graph_kronecker_whole_df)\n",
    "\n",
    "# for each geoid_o which is a key in out_dict, get the lat_o and lng_o from the graph_kronecker_whole_df\n",
    "# then plot the united states and put the values of out_dict as the node values (new_confirmed)\n",
    "# then compare with the target_dict values\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes\n",
    "geoid_o = list(out_dict.keys())\n",
    "lat_o = [graph_kronecker_whole_df[graph_kronecker_whole_df['geoid_o'] == key]['lat_o'].values[0] for key in out_dict.keys()]\n",
    "lng_o = [graph_kronecker_whole_df[graph_kronecker_whole_df['geoid_o'] == key]['lng_o'].values[0] for key in out_dict.keys()]\n",
    "\n",
    "\n",
    "# cumulative_confirmed = [out_dict[key] for key in out_dict.keys()] # out_dict[key] will return a dictionary not a value\n",
    "\n",
    "cumulative_confirmed = []\n",
    "# pull column 3 from graph_signal_matrix as cumulative_confirmed\n",
    "for i in range(model.graph_signal_matrix.shape[0]):\n",
    "    cumulative_confirmed.append(model.graph_signal_matrix[i, 2])\n",
    "\n",
    "\n",
    "for i in range(len(geoid_o)):\n",
    "    G.add_node(geoid_o[i], cumulative_confirmed=cumulative_confirmed[i], lat=lat_o[i], lng=lng_o[i])\n",
    "\n",
    "# Get node positions\n",
    "pos = {node: (G.nodes[node]['lng'], G.nodes[node]['lat']) for node in G.nodes}\n",
    "\n",
    "# Get node values\n",
    "node_values = [G.nodes[node]['cumulative_confirmed'] for node in G.nodes]\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw_networkx(G, pos, node_color=node_values, cmap='viridis', node_size=node_values, alpha=0.8, with_labels=False)\n",
    "# plt.colorbar(label='New Confirmed Cases')\n",
    "plt.title('Graph of COVID-19 Cases: MODEL OUTPUT')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes\n",
    "geoid_o = list(out_dict.keys())\n",
    "lat_o = [graph_kronecker_whole_df[graph_kronecker_whole_df['geoid_o'] == key]['lat_o'].values[0] for key in out_dict.keys()]\n",
    "lng_o = [graph_kronecker_whole_df[graph_kronecker_whole_df['geoid_o'] == key]['lng_o'].values[0] for key in out_dict.keys()]\n",
    "\n",
    "\n",
    "# cumulative_confirmed = [out_dict[key] for key in out_dict.keys()] # out_dict[key] will return a dictionary not a value\n",
    "\n",
    "cumulative_confirmed = []\n",
    "# pull column 3 from graph_signal_matrix as cumulative_confirmed\n",
    "for i in range(model.target_graph_signal_matrix.shape[0]):\n",
    "    cumulative_confirmed.append(model.target_graph_signal_matrix[i, 2])\n",
    "\n",
    "for i in range(len(geoid_o)):\n",
    "    G.add_node(geoid_o[i], cumulative_confirmed=cumulative_confirmed[i], lat=lat_o[i], lng=lng_o[i])\n",
    "\n",
    "# Get node positions\n",
    "pos = {node: (G.nodes[node]['lng'], G.nodes[node]['lat']) for node in G.nodes}\n",
    "\n",
    "# Get node values\n",
    "node_values = [G.nodes[node]['cumulative_confirmed'] for node in G.nodes]\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw_networkx(G, pos, node_color=node_values, cmap='viridis', node_size=node_values, alpha=0.8, with_labels=False)\n",
    "# plt.colorbar(label='New Confirmed Cases')\n",
    "plt.title('Graph of COVID-19 Cases: Ground Truth')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
